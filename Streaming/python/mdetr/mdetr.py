# -*- coding: utf-8 -*-
"""MDETR_demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/ashkamath/mdetr/blob/colab/notebooks/MDETR_demo.ipynb

# MDETR - Modulated Detection for End-to-End Multi-Modal Understanding

Welcome to the demo notebook for MDETR. We'll show-case detection, segmentation and question answering

## Preliminaries

This section contains the initial boilerplate. Run it first.
"""
import time
import torch
from PIL import Image
import requests
import torchvision.transforms as T
import matplotlib.pyplot as plt
from collections import defaultdict
import torch.nn.functional as F
import numpy as np
from skimage.measure import find_contours

from matplotlib import patches,  lines
from matplotlib.patches import Polygon

torch.set_grad_enabled(False)


has_gpu = torch.cuda.is_available()

def maybe_cuda(x):
    return x.cuda() if has_gpu else x

def cycle(xs, i):
    return xs[i%len(xs)]


class MDetr:
    org = 'ashkamath/mdetr:main'
    name = 'mdetr_efficientnetB5'

    default_url = "http://images.cocodataset.org/val2017/000000281759.jpg"
    default_captions = [
        "5 people each holding an umbrella",
        "A green umbrella. A pink striped umbrella. A plain white umbrella",
        "a flowery top. A blue dress. An orange shirt",
        "a car. An electricity box",
        "a koala",
    ]

    colors = [
        [0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
        [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]

    def __init__(self):
        model, postprocessor = torch.hub.load(self.org, self.name, pretrained=True, return_postprocessor=True)
        model = maybe_cuda(model)
        model.eval()
        self.model = model
        self.postprocessor = postprocessor

    def predict(self, im, caption, confidence=0.7):
        # mean-std normalize the input image (batch-size: 1)
        img = maybe_cuda(transform(im).unsqueeze(0))
        memory_cache = self.model(img, [caption], encode_and_save=True)
        outputs = self.model(img, [caption], encode_and_save=False, memory_cache=memory_cache)

        logits = outputs["pred_logits"][0].softmax(-1).cpu()
        probas = 1 - logits[:, -1]

        # convert boxes from [0; 1] to image scales
        bboxes = box_cxcywh_to_xyxy(outputs['pred_boxes'][0].cpu())
        keep = nms(bboxes.numpy(), probas.numpy(), min_score=confidence)
        bboxes = rescale_bboxes(bboxes[keep], im.size)

        # Extract the text spans predicted by each box
        labels = self._get_labels(logits[keep], caption, memory_cache["tokenized"])
        return probas[keep], bboxes, labels

    def _get_labels(self, logits, caption, tokenized, threshold=0.1):
        positive_tokens = (logits > threshold).nonzero().tolist()
        predicted_spans = defaultdict(str)
        for item, pos in positive_tokens:
            if pos < 255:
                span = tokenized.token_to_chars(0, pos)
                predicted_spans[item] += " " + caption[span.start:span.end]
        return [predicted_spans[k] for k in sorted(predicted_spans)]

    def draw(self, im, caption, *dets):
        plt.figure(figsize=(16,10))
        im = np.array(im)
        ax = plt.gca()
        print([len(x) for x in dets])
        assert len(set(len(x) for x in dets)) == 1, 'all outputs need to be the same size'
        for i, det in enumerate(zip(*dets)):
            print(i, len(det))
            self.draw_detection(ax, im, i, *det)
        plt.imshow(im)
        ax.text(0, 0, caption, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))
        plt.axis('off')
        plt.show()

    def draw_detection(self, ax, im, i, score, box, label):
        xmin, ymin, xmax, ymax = box
        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin, fill=False, color=cycle(self.colors, i), linewidth=3))
        ax.text(xmin, ymin, f'{label}: {score:0.2f}', fontsize=15, bbox=dict(facecolor='white', alpha=0.8))


class MDetrCut(MDetr):
    name = 'mdetr_efficientnetB3_phrasecut'

    default_url = "https://s3.us-east-1.amazonaws.com/images.cocodataset.org/val2017/000000218091.jpg"
    default_captions = [
        "bed", "phone", "lamp", 
        'door', 
        'couch', 'tv', 'mirror'
    ]

    def predict(self, im, caption, confidence=0.9):
        # mean-std normalize the input image (batch-size: 1)
        img = maybe_cuda(transform(im).unsqueeze(0))
        outputs = self.model(img, [caption])

        # keep only predictions with 0.9+ confidence
        logits = outputs["pred_logits"][0].softmax(-1).cpu()
        probas = 1 - logits[:, -1]

        # convert boxes from [0; 1] to image scales
        bboxes = box_cxcywh_to_xyxy(outputs['pred_boxes'][0].cpu())
        keep = nms(bboxes.numpy(), probas.numpy(), min_score=confidence)
        bboxes = rescale_bboxes(bboxes[keep], im.size)

        # Interpolate masks to the correct size
        masks = F.interpolate(outputs["pred_masks"][:, keep], size=im.size[::-1], mode="bilinear", align_corners=False)[0]
        masks = masks.cpu().sigmoid() > 0.5

        # Extract the text spans predicted by each box
        tokenized = self.model.detr.transformer.tokenizer.batch_encode_plus(
            [caption], padding="longest", return_tensors="pt").to(img.device)
        labels = self._get_labels(logits[keep], caption, tokenized)
        return probas[keep], bboxes, labels, masks

    def draw_detection(self, ax, im, i, score, box, label, mask=None):
        super().draw_detection(ax, im, i, score, box, label)
        if mask is None:
            return
        c = cycle(self.colors, i)
        im = apply_mask(im, mask, c)
        contours = find_contours(np.pad(mask, ((1, 1), (1, 1))).astype('uint8'), 0.5)
        for verts in contours:  # Subtract the padding and flip (y, x) to (x, y)
            ax.add_patch(Polygon(np.fliplr(verts) - 1, facecolor="none", edgecolor=c))



class MDetrQA(MDetr):
    name = 'mdetr_efficientnetB5_gqa'
    ans_types = ["obj", "attr", "rel", "global", "cat"]
    
    default_url = "https://s3.us-east-1.amazonaws.com/images.cocodataset.org/val2017/000000076547.jpg"
    default_captions = [
        "What color is the train?",
        "What is on the table?",
        "What is tied to the bike?",
        # "What numbers are on the pole?",  # no
        # "What color is the coffee sign?",  # ya
        # "What time does the orange clock say?",  # no
    ]

    answer_json_url = "https://nyu.box.com/shared/static/j4rnpo8ixn6v0iznno2pim6ffj3jyaj8.json"

    def __init__(self):
        super().__init__()
        import json
        self.id2answer = {
            t: {v: k for k, v in a2i.items()} 
            for t, a2i in json.load(requests.get(self.answer_json_url, stream=True).raw).items()
        }      

    def predict(self, im, caption, confidence=0.7):
        # mean-std normalize the input image (batch-size: 1) # propagate through the model
        img = maybe_cuda(transform(im).unsqueeze(0))
        memory_cache = self.model(img, [caption], encode_and_save=True)
        outputs = self.model(img, [caption], encode_and_save=False, memory_cache=memory_cache)

        # keep only predictions with 0.9+ confidence
        logits = outputs["pred_logits"][0].softmax(-1).cpu()
        probas = 1 - logits[:, -1]

        # convert boxes from [0; 1] to image scales
        bboxes = box_cxcywh_to_xyxy(outputs['pred_boxes'][0].cpu())
        keep = nms(bboxes.numpy(), probas.numpy(), min_score=confidence)
        bboxes = rescale_bboxes(bboxes[keep], im.size)

        # Extract the text spans predicted by each box
        labels = self._get_labels(logits[keep], caption, memory_cache["tokenized"])

        # Classify the question type
        answer, type_conf, ans_conf = self._classify_question(outputs)
        ans_conf = (type_conf * ans_conf).item()
        print(f"Predicted answer: {answer}\t confidence={ans_conf:.2%}")
        return probas[keep], bboxes, labels, (answer, ans_conf)

    def _classify_question(self, outputs):
        type_conf, type_pred = outputs["pred_answer_type"].softmax(-1).max(-1)
        ans_type = self.ans_types[type_pred.item()]
        ans_conf, ans = outputs[f"pred_answer_{ans_type}"][0].softmax(-1).max(-1)
        answer = self.id2answer[f"answer_{ans_type}"][ans.item()]
        return answer, type_conf, ans_conf

    def draw(self, im, caption, *a):
        answer, ans_conf = a[-1]
        super().draw(im, f'{caption}    {answer} ({ans_conf:.2%})', *a[:-1])





def main(*captions, url=None, model=None):
    m = MDetrQA() if model == 'qa' else MDetrCut() if model == 'mask' else MDetr()
    url = url or m.default_url
    
    im = Image.open(requests.get(url, stream=True).raw)
    for caption in captions or m.default_captions:
        t0 = time.time()
        outs = m.predict(im, caption)
        print(len(outs), [type(o) for o in outs])
        print(f'predict took {time.time() - t0:.3f} seconds')
        m.draw(im, caption, *outs)




# standard PyTorch mean-std input image normalization
transform = T.Compose([
    T.Resize(800),
    T.ToTensor(),
    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])

# for output bounding box post-processing
def box_cxcywh_to_xyxy(x):
    x_c, y_c, w, h = x.unbind(1)
    return torch.stack([
        (x_c - 0.5 * w), (y_c - 0.5 * h),
        (x_c + 0.5 * w), (y_c + 0.5 * h)], dim=1)

def rescale_bboxes(b, size):
    img_w, img_h = size
    # b = box_cxcywh_to_xyxy(out_bbox)
    return b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)

# colors for visualization
COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],
          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]

def apply_mask(image, mask, color, alpha=0.5):
    """Apply the given mask to the image."""
    for c in range(3):
        im = image[:, :, c]
        image[:, :, c] = np.where(mask == 1, im * (1 - alpha) + alpha * color[c] * 255, im)
    return image



def nms(boxes, scores, threshold=0.3, min_box_area=0, min_score=0.7, max_dets=1000):
    """Non-maximum suppression.
    Args:
        dets: [N, 4]
        scores: [N,]
        thresh: iou threshold. Float
        max_dets: int.
    """
    x1, y1, x2, y2 = boxes.T
    areas = (x2 - x1) * (y2 - y1)
    order = (-scores).argsort()
    order = order[(scores[order] >= min_score) & (areas[order] > min_box_area)]

    keep = []
    while len(order) > 0 and len(keep) < max_dets:
        i, js = order[0], order[1:]
        keep.append(i)

        # check overlap
        w = np.maximum(0.0, np.minimum(x2[i], x2[js]) - np.maximum(x1[i], x1[js]))
        h = np.maximum(0.0, np.minimum(y2[i], y2[js]) - np.maximum(y1[i], y1[js]))
        intersect = w * h
        overlap = intersect / (areas[i] + areas[js] - intersect + 1e-12)
        order = order[np.where(overlap <= threshold)[0] + 1]

    return np.array(keep)
    # nmsed = np.zeros(len(scores), dtype=bool)
    # nmsed[keep] = True
    # print(nmsed)
    # return np.where(nmsed)[0]


if __name__ == '__main__':
    import fire
    fire.Fire(main)